---
output: github_document
title: "Lab 7"
author: "Aiden Kelly, Brandon Kim, Sam Todd"
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# clust431

<!-- badges: start -->
<!-- badges: end -->

The goal of clust431 is to provide functions that implement a k-means algorithm and perform agglomerative hierarchical clustering. 

## Installation

You can install the released version of clust431 from [CRAN](https://CRAN.R-project.org) with:

```{r install package, eval=FALSE}
install.packages("clust431")
```

```{r load package}
library(clust431)
library(tidyverse) # <- for examples
```

## K-Means Example

This is a basic example which shows you how to use the k_means() function to obtain cluster assignments for a data set. 

```{r}
set.seed(20)
iris_km <- iris %>%
    select(Petal.Length, Petal.Width) %>%
    k_means(k = 3)
iris_km
```

In this example, we performed k means clustering on the Iris dataset, using petal length and width. The function output gives us a vector of each group that the flowers are in, the sum of squares for each group, and the centroids of each group. Since we specified k = 3, we can see that it made 3 clusters.

We will see how accurately our k_means algorithm clustered the species together

```{r}
iris %>%
    mutate(cluster = as.character(iris_km$clustering_vector)) %>%
    ggplot(aes(x = Petal.Length, y = Petal.Width, color = cluster)) +
    geom_point()
```

```{r}
iris %>%
    ggplot(aes(x = Petal.Length, y = Petal.Width, color = Species)) +
    geom_point()
```

As you can see, it was able to predict the species pretty well. 

We can also apply PCA to the same data as well, and use all the variables. 

```{r}
iris %>%
    select(-Species) %>%
    k_means(k = 3, PCA = T)
```


## Hierarchical Clustering Example

This is a basic example which shows you how to use the hier_clust() function to obtain cluster assignments for a data set. 

```{r}
# first 10 observations from the iris dataset
data <- iris |> 
    head(10)

# remove the categorical variable
# NOTE: the function will automatically select only numeric variables anyway 
data <- subset(data, select = -Species)

# perform hierarchical clustering
iris_hc <- hier_clust(data, 3)

# output the cluster assignments
iris_hc
```

In this example, we perform hierarchical clustering on the first 10 observations from the iris dataset. The function output gives us the cluster assignments for the data we provided. From the output we can see that the function successfully grouped the data into clusters as we supplied k=3 to the function.

```{r}
# save a copy without cluster assignments
data2 <- data

# add a new row with the cluster assignments to the original dataset
data$cluster <- iris_hc

# show the new dataset
data
```

If we attach the cluster assignments to the original dataset, we can see that the assignments make sense. Observation 6 is the only observation in its own cluster. Since it has the largest value for every variable in the dataset, it make sense that it did find another cluster or observation that was similar to it. 

```{r}
# calculate the distance matrix
iris_dist <- dist(data2)

# output matrix
iris_dist
```

Looking at the distance matrix above, the smallest distance is 0.1414 between observations 1 and 5. We can confirm this by asking hier_clust to only perform the first iteration of clustering and seeing if those two observations are put into the same cluster.

```{r}
# perform only 1 iteration of hier_clust()
iris_hc_2 <- hier_clust(data2, nrow(data2)-1)

# output the cluster assignments
iris_hc_2
```

As we can see from the hier_clust() output, the 1st and 5th observation were put in the same cluster after only one iteration. Therefore, the function is working as expected.
